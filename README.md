# Tarantulla

> **NOTE**: This is a _alpha_ version which still needs some adaptation. A lot of persistence, coupling and manual work to be generalized and automated. See our [Issues](https://github.com/oncase/tarantulla/issues) and [Milestones](https://github.com/oncase/tarantulla/issues?q=is%3Aopen+is%3Aissue+no%3Amilestone) to see what are our intentions. Be patient: soon we'll publish about deploy and utilization.

Tarantulla organized data and metadata of several _sources_ (Publishers) in an unified database for further analysis. The process of consolidation is organized in 3 steps:

* Links' capture;
* Content download;
* _Parse_ of content;
* Enrichment of content engagement;

## Engine

_Crawling_ implies specific implementation for each _source_ (publisher). Our engine delegate those specificities are describe at the integration contract - as in java, imagine that you need to implement some _interfaces_.

Each _source_, to comply the contract, needs to:
 
 - Implement a `sources/<SOURCE-NAME>/getLinks.kjb` - the job needs to copy from _resultset's_ fields:
    * **url** - Content's address;
    * **source** - Publisher's name.
 - Implement um `sources/<SOURCE-NAME>/parseHTML.ktr` - the transformation needs to query the table `staging.sources_html` and copy for _resultset_:
    * **url** - Content's address
    * **source** - Publisher's name
    * **tags** - Content's tags separate by `,`;
    * **date** - publish date from content - `yyyy-MM-dd`;
    * **title** - Page's title;
    * **textHTML** - HTML's content;
    * **author** - Content's author.
 - Implement config.json with attributes:
    * **name** - Publisher unique name. ie.: `Gizmodo B`;
    * **brand** - The generic brand aggregate all publishers from them ie.: `Gizmodo`;
    * **lang** - Publisher's language. ie.: `en`;
    * **locale** - Publisher's locale. ie.: `US`;
    * **category** - Category for aggregate of publisher and brands under, ie. `Technology`.
    
    

### Staging area

Since the process can be time consuming, because of all necessary downloads, bandwidth limitations and websites' access, our engine tries to not waste time with unnecessary efforts. That's why,  you need to understand the stagings' role.

We have 3 staging tables until our parse:

* **staging.sources_links**
  - store all links processed;
  - link is _primary key_;
* **staging.sources_html**
  - contains all the fields from before, plus downloaded HMTL's content; 
  - link is _primary key_;
  - Before download the HTML, the engine verifies if exists any HTML for that link;
* **staging.sources_parsed** - 
  - contains all sources_links' fields, plus all the fields generated by `parseHTML`; 
  - link is _primary key_;
  - at `parseHTML.ktr`, the engine most bring from `sources_html`, only the content that doesn't exists in `staging.sources_parsed`;
* **staging.sources_facebook** - 
  - contains all the _sources_links_'s fields, plus engagements' fields from query https://developers.facebook.com/docs/graph-api/reference/v2.8/url;
  - link is _primary key_;
  - the engine brings from `sources_parsed` only content that doesn't exists in `staging.sources_facebook`;
  - if daily limits of queries is reach, sometimes we need to do:

  ```sql
  delete from staging.sources_facebook where response like '%User request limit reached%'; 
  ```

This way, even if we get some errors from any staging, the engine tends to be able to execute from where it stopped, with reduced damage.

## Utilização

We have 5 of main execution parameters from `./main.kjb`:

---

`-param:fetchLinks=(true|false)`

If we need to execute links' discovery step for all sources;

---

`-param:downloadContent=(true|false)`

If we need to execute content's download step of all existing non downloaded links, which isn't cached in `sources_html`).

---

`-param:parseContent=(true|false)`

If we need to execute content's parse step of all existing persisted links and  wasn't converted, which isn't cached in `sources_parsed`).
---

`-param:fetchFacebookEngagement=(true|false)`

If we need to execute Facebook's engagement step of all existing links without data, which isn't cached in `sources_facebook`).

---

`-param:spscific=(NOTSET|<ID-DO-SOURCE>)`

If specified, our engine will execute just for this source. `<ID-DO-SOURCE>` is a folder's name where the source's implementation is storaged, besides `sources/`. Leave it blank if no specification is needed. 


# Backup/restore

For larger backups it is preferable to use `pg_dump` in command line.

## Backup

For the compressed backup of 3 tables from _Market_:

```bash
./pg_dump -U postgres \
--table=staging.sources_links \
--table=staging.sources_html \
--table=staging.sources_parsed \
--table=staging.sources_facebook \
--no-privileges \
--no-owner \
--compress=9 \
-Fc \
--no-tablespaces tarantulla > ./tarantulla.postgres.dump
```

## restore

For restore a file generated from previous command:

```bash
./pg_restore -U postgres \
--dbname=tarantulla \
--verbose \
-Fc \
./tarantulla.postgres.dump
```
